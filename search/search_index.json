{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seja Bem Vindo","text":"<p> <p> </p>  Ol\u00e1!   Meu nome \u00e9 Pedro, sou um entusiasta da tecnologia e adoro estudar a sociedade e seus padr\u00f5es.    O prop\u00f3sito desta documenta\u00e7\u00e3o \u00e9 compartilhar a trajet\u00f3ria que percorri para obter a certifica\u00e7\u00e3o Databricks Certified Associate Developer for Apache Spark 3.0. Aqui, relato conceitos que estudei, materiais que consultei na internet, recomenda\u00e7\u00f5es de boas pr\u00e1ticas, orienta\u00e7\u00f5es para configurar o cluster, insights sobre a arquitetura, simulados que realizei, entre outras sugest\u00f5es para certifica\u00e7\u00e3o e para o uso spark.     Nesta documenta\u00e7\u00e3o, voc\u00ea n\u00e3o encontrar\u00e1 discuss\u00f5es profundas sobre os t\u00f3picos, mas sim uma vis\u00e3o geral dos temas relacionados \u00e0 certifica\u00e7\u00e3o, bem como refer\u00eancias para fontes que detalham mais sobre cada assunto. Considerando a vasta quantidade de excelentes recursos dispon\u00edveis online, meu objetivo \u00e9 oferecer uma compreens\u00e3o abrangente dos conceitos e direcion\u00e1-lo a recursos onde voc\u00ea possa se aprofundar ainda mais.   <p> </p>"},{"location":"apache_spark/","title":"Apache Spark","text":""},{"location":"apache_spark/#apache-spark","title":"Apache Spark","text":"O Apache Spark \u00e9 um framework de c\u00f3digo aberto destinado \u00e0 computa\u00e7\u00e3o distribu\u00edda. Originou-se no AMPLab da Universidade da Calif\u00f3rnia e, posteriormente, foi transferido para a Apache Software Foundation, que tem gerenciado o projeto desde ent\u00e3o. O Spark oferece uma interface para programa\u00e7\u00e3o de clusters, garantindo paralelismo e toler\u00e2ncia a falhas.   Componentes do Spark:  - Spark Core: Base do framework, respons\u00e1vel pelo agendamento de tarefas, gerenciamento de mem\u00f3ria e intera\u00e7\u00f5es com sistemas de armazenamento. - Spark SQL: Facilita a execu\u00e7\u00e3o de consultas SQL em dados distribu\u00eddos e \u00e9 compat\u00edvel com diversas fontes de dados, como Hive, Avro, Parquet e JSON. - Spark Streaming: Dedicado ao processamento de dados em tempo real. - MLlib: Biblioteca para aprendizado de m\u00e1quina. - GraphX: Voltada para o processamento de gr\u00e1ficos distribu\u00eddos e computa\u00e7\u00e3o paralela."},{"location":"apache_spark/#caracteristicas-do-spark","title":"Caracter\u00edsticas do Spark:","text":"- Funciona tanto em ambientes locais quanto em nuvem. - Processa dados de forma distribu\u00edda. - Apresenta uma interface intuitiva e f\u00e1cil de usar. - Baseia-se no Resilient Distributed Dataset (RDD). - Otimizado para alta performance, sendo ideal para an\u00e1lises em big data. - Modular e extens\u00edvel. - Software de c\u00f3digo aberto. - Integra-se com v\u00e1rias bibliotecas, abrangendo Machine Learning, processamento de streaming, SQL, GraphX e outras. - Suporta variados formatos de dados, incluindo CSV, JSON, parquet e Avro. - Adota o conceito de Lazy Evaluation. - Utiliza a conven\u00e7\u00e3o de nomenclatura camelCase. - Compat\u00edvel com linguagens de programa\u00e7\u00e3o como Scala, Python, Java e R."},{"location":"apache_spark/#referencias","title":"Refer\u00eancias","text":"<p>Wikipedia Apache Spark O que \u00e9 Apache Spark Google</p>"},{"location":"aqe/","title":"Adaptive Query Execution","text":"O Adaptive Query Execution (AQE) \u00e9 uma caracter\u00edstica inovadora do Apache Spark que tem como principal objetivo otimizar consultas em tempo real. Ele opera monitorando e coletando estat\u00edsticas sobre os dados e o ambiente durante a execu\u00e7\u00e3o. Baseando-se nessas informa\u00e7\u00f5es, o AQE implementa otimiza\u00e7\u00f5es para melhorar a efici\u00eancia da consulta. Introduzido na vers\u00e3o 3.0 do Spark, o AQE apresenta v\u00e1rias capacidades, sendo uma das mais not\u00e1veis a \"Coalesc\u00eancia Din\u00e2mica de Parti\u00e7\u00f5es para Shuffle\" (Dynamically Coalescing Shuffle Partitions)."},{"location":"aqe/#coalescencia-dinamica-de-particoes-para-shuffle","title":"Coalesc\u00eancia Din\u00e2mica de Parti\u00e7\u00f5es para Shuffle:","text":"A opera\u00e7\u00e3o de \"shuffle\" \u00e9 uma das mais custosas no Spark, principalmente devido \u00e0 necessidade de redistribui\u00e7\u00e3o de dados entre parti\u00e7\u00f5es. O AQE aborda esse desafio otimizando o shuffle. Ele agrupa dinamicamente parti\u00e7\u00f5es que s\u00e3o menores e geograficamente pr\u00f3ximas, minimizando o volume de dados trocados. Esta otimiza\u00e7\u00e3o \u00e9 ilustrada por meio de um diagrama visual, mostrando a estrutura das parti\u00e7\u00f5es antes e depois do processo de coalesc\u00eancia."},{"location":"aqe/#troca-dinamica-de-estrategias-de-join","title":"Troca Din\u00e2mica de \u201cEstrat\u00e9gias de Join\u201d","text":"Ao executar uma opera\u00e7\u00e3o de \"join\" no Spark, diferentes estrat\u00e9gias s\u00e3o aplicadas em segundo plano para otimizar a opera\u00e7\u00e3o. Entre as estrat\u00e9gias mais comuns, destacam-se o \"Broadcast Hash Join\" e o \"Shuffle Sort Merge Join\". O \"Broadcast Hash Join\" \u00e9 geralmente considerado mais eficiente e \u00e9 frequentemente a primeira escolha, especialmente quando um dos datasets \u00e9 significativamente menor que o outro. No entanto, h\u00e1 situa\u00e7\u00f5es em que o \"Shuffle Sort Merge Join\" pode ser mais adequado.  Dynamically Switching Join Strategies: Com o recurso Adaptive Query Execution (AQE), o Spark tem a capacidade de alternar dinamicamente entre essas estrat\u00e9gias de \"join\" com base nas estat\u00edsticas dos dados em tempo real. Assim, se inicialmente o Spark escolhe uma estrat\u00e9gia, mas ao coletar estat\u00edsticas durante a execu\u00e7\u00e3o percebe que outra estrat\u00e9gia seria mais eficiente, ele pode mudar para essa estrat\u00e9gia. Isso proporciona uma execu\u00e7\u00e3o mais otimizada, adaptando-se dinamicamente \u00e0s caracter\u00edsticas dos dados e \u00e0s necessidades da opera\u00e7\u00e3o de \"join\"."},{"location":"aqe/#dynamically-optimizing-skew-joins","title":"Dynamically Optimizing Skew Joins","text":"\"Joins inclinados\" ou \"Skew Joins\" referem-se ao cen\u00e1rio onde, durante uma opera\u00e7\u00e3o de \"join\", uma ou mais parti\u00e7\u00f5es s\u00e3o significativamente maiores do que as outras, levando a desequil\u00edbrios na carga de trabalho distribu\u00edda. Esses desequil\u00edbrios podem ser problem\u00e1ticos, pois parti\u00e7\u00f5es maiores levam mais tempo para serem processadas, tornando-as pontos de estrangulamento que atrasam a execu\u00e7\u00e3o completa de jobs.  <p> Com o Adaptive Query Execution (AQE) do Spark, \u00e9 poss\u00edvel abordar dinamicamente esse problema. Quando o Spark detecta que uma parti\u00e7\u00e3o \u00e9 significativamente maior e pode causar um \"join inclinado\", ele automaticamente quebra a parti\u00e7\u00e3o grande em parti\u00e7\u00f5es menores, a fim de distribuir a carga de trabalho de forma mais uniforme entre os executores. Isso aumenta o paralelismo e melhora a efici\u00eancia geral da opera\u00e7\u00e3o. Para visualizar isso, imagine um gr\u00e1fico ou uma ilustra\u00e7\u00e3o que mostre um \"join\" com a parti\u00e7\u00e3o A0, significativamente maior que a parti\u00e7\u00e3o B0. Ap\u00f3s a otimiza\u00e7\u00e3o com AQE, A0 \u00e9 quebrado em v\u00e1rias parti\u00e7\u00f5es menores, cada uma se </p> <p> </p>"},{"location":"aqe/#ganhos-de-desempenho-do-tpc-ds-do-aqe","title":"Ganhos de desempenho do TPC-DS do AQE","text":""},{"location":"aqe/#para-ativar-no-spark","title":"Para ativar no Spark","text":"O Adaptive Query Execution (AQE) \u00e9 uma poderosa funcionalidade do Spark 3.0 que, quando ativada, pode oferecer otimiza\u00e7\u00f5es significativas em tempo real durante a execu\u00e7\u00e3o das consultas. Para habilitar o AQE, \u00e9 necess\u00e1rio ajustar uma configura\u00e7\u00e3o espec\u00edfica no Spark.  Como Ativar: <p>Configure o par\u00e2metro <code>spark.sql.adaptive.enabled</code> para <code>true</code>. Vale ressaltar que o valor padr\u00e3o dessa configura\u00e7\u00e3o no Spark 3.0 \u00e9 <code>false</code>.</p>"},{"location":"aqe/#referencias","title":"Refer\u00eancias:","text":"<p>Adaptive Query Execution: Speeding Up Spark SQL at Runtime - Databricks AQE Demo - Databricks Spark 3.0 Feature: Dynamic Partition Pruning (DPP) to Avoid Scanning Irrelevant Data - Medium Spark 3.0: Adaptive Query Execution &amp; Dynamic Partition Pruning</p>"},{"location":"arquitetura/","title":"Arquitetura","text":"Vamos abordar aqui a arquitetura tendo como perspectiva dois n\u00edveis de paraleliza\u00e7\u00e3o s\u00e3o eles o de dados e o de tarefas. Vamos iniciar com o de dados, pois ele trata sobre a infraestrutura. O primeiro ponto que temos que trazer \u00e9 que o Spark \u00e9 um cont\u00eainer Java Virtual Machine (JVM) = O. Eu tive essa rea\u00e7\u00e3o quando descobri isso.   Java Virtual Machine (JVM): A JVM \u00e9 uma m\u00e1quina virtual que permite que um computador execute aplica\u00e7\u00f5es Java. Ela converte o bytecode Java, que \u00e9 independente de plataforma, em instru\u00e7\u00f5es espec\u00edficas da m\u00e1quina para execu\u00e7\u00e3o. Isso permite que o c\u00f3digo Java seja escrito uma vez e executado em qualquer dispositivo ou sistema operacional que tenha uma JVM.  Containers: Containers s\u00e3o uma tecnologia de virtualiza\u00e7\u00e3o que permite empacotar uma aplica\u00e7\u00e3o e todas as suas depend\u00eancias, bibliotecas e configura\u00e7\u00f5es em um \u00fanico pacote. Ao contr\u00e1rio das m\u00e1quinas virtuais tradicionais, os containers compartilham o mesmo sistema operacional do host, mas t\u00eam espa\u00e7os de execu\u00e7\u00e3o isolados. Isso os torna leves e r\u00e1pidos.  Outra informa\u00e7\u00e3o relevante sobre \u00e9 que a paraleliza\u00e7\u00e3o de dados envolve dividir os dados em m\u00faltiplas parti\u00e7\u00f5es, permitindo que cada uma seja processada de forma independente em n\u00f3s distintos de um cluster. No Spark, essa abordagem possibilita a execu\u00e7\u00e3o simult\u00e2nea de opera\u00e7\u00f5es em diversas parti\u00e7\u00f5es, otimizando consideravelmente a velocidade de processamento."},{"location":"arquitetura/#nivel-de-paralelizacao-de-dados","title":"N\u00edvel de paraleliza\u00e7\u00e3o de dados","text":"Dado essa contextualiza\u00e7\u00e3o, podemos introduzir a infraestrutura do spark. Abaixo temos algumas imagens de como essa arquitetura \u00e9 representada:   Vamos explorar os seguintes itens dos desenhos acima: sparkSession, Spark Driver, Cluster Manager, Executer, Slot"},{"location":"arquitetura/#spark-session","title":"Spark Session","text":"A Spark Session funciona como o principal ponto de entrada para interagir com as funcionalidades do Apache Spark. Oferecendo uma interface global para realizar tarefas rotineiras, facilita a intera\u00e7\u00e3o com os dados e assegura uma abordagem uniforme ao seu processamento. Por meio da Spark Session, os usu\u00e1rios podem executar SQL, Rspark, Pyspark e Scala em seus conjuntos de dados."},{"location":"arquitetura/#spark-driver","title":"Spark Driver","text":"Principais responsabilidades do Spark Driver: <p>Cria\u00e7\u00e3o do Spark Session/Context. An\u00e1lise e otimiza\u00e7\u00e3o do plano de execu\u00e7\u00e3o. Segmenta\u00e7\u00e3o em tarefas. Distribui\u00e7\u00e3o do c\u00f3digo para os executores. Gerenciamento do DAG e da linhagem (Lineage). Coordena\u00e7\u00e3o dos executores. Coleta dos resultados. Monitoramento e recupera\u00e7\u00e3o em caso de falhas. Finaliza\u00e7\u00e3o e encerramento da aplica\u00e7\u00e3o. Atua como o n\u00f3 mestre no contexto de processamento distribu\u00eddo.</p> Resumo: O Spark Driver atua como o cora\u00e7\u00e3o da aplica\u00e7\u00e3o Spark, tomando conta de todo o processamento. Ele coordena a execu\u00e7\u00e3o em um ambiente de cluster, otimiza o plano de execu\u00e7\u00e3o e garante confiabilidade e toler\u00e2ncia a falhas durante o tratamento dos dados."},{"location":"arquitetura/#cluster-manager","title":"Cluster Manager","text":"Principais responsabilidades do Cluster Manager: <p>Aloca\u00e7\u00e3o de recursos. Monitoramento dos recursos.</p> Resumo: O Cluster Manager do Spark desempenha uma fun\u00e7\u00e3o vital no gerenciamento de recursos e assegura a execu\u00e7\u00e3o eficiente dos aplicativos Spark em um ambiente de cluster distribu\u00eddo.  Curiosidade: O Spark suporta diversos gerenciadores de cluster, como seu pr\u00f3prio gerenciador Standalone, bem como Apache Mesos, Hadoop YARN e Kubernetes."},{"location":"arquitetura/#workersexecutores","title":"Workers/Executores","text":"Principais atividades dos Workers/Executores: <p>Execu\u00e7\u00e3o de tarefas. Processamento paralelo. Acesso a dados, seja em mem\u00f3ria ou em disco. Realiza\u00e7\u00e3o de transforma\u00e7\u00f5es e a\u00e7\u00f5es. Gerenciamento de parti\u00e7\u00f5es. Comunica\u00e7\u00e3o com o Spark Driver.</p> Resumo: O executor, tamb\u00e9m conhecido como worker, tem a crucial responsabilidade de conduzir o processamento distribu\u00eddo real no Spark. Ele executa tarefas, processa dados em paralelo e mant\u00e9m comunica\u00e7\u00e3o cont\u00ednua com o Spark Driver. Essas a\u00e7\u00f5es garantem efici\u00eancia na execu\u00e7\u00e3o e capacidade de recupera\u00e7\u00e3o diante de poss\u00edveis falhas no ambiente distribu\u00eddo.  <p></p>"},{"location":"arquitetura/#slots","title":"Slots","text":"Um slot \u00e9 a menor unidade de paralelismo em dados no Spark. O n\u00famero de slots em um executor determina quantas tarefas podem ser realizadas simultaneamente. Por exemplo, se um executor possui 4 slots, ele pode processar at\u00e9 4 tarefas ao mesmo tempo, desde que estas tarefas estejam prontas para serem executadas. Os slots s\u00e3o usados para regular a quantidade de recursos, tais como CPU e mem\u00f3ria, que s\u00e3o destinados a cada tarefa.  Resumo: Os slots t\u00eam uma import\u00e2ncia vital na determina\u00e7\u00e3o do paralelismo e no gerenciamento de recursos no Apache Spark. Eles asseguram que o Spark utilize de maneira eficaz os recursos dispon\u00edveis no cluster, possibilitando o processamento paralelo de tarefas e garantindo efici\u00eancia e escalabilidade em um ambiente distribu\u00eddo."},{"location":"arquitetura/#nivel-de-paralelizacao-das-tarefas","title":"N\u00edvel de paraleliza\u00e7\u00e3o das tarefas","text":"Vamos discutir o n\u00edvel de paraleliza\u00e7\u00e3o das tarefas. Mas, primeiramente, o que significa esse n\u00edvel? A paraleliza\u00e7\u00e3o de tarefas refere-se \u00e0 execu\u00e7\u00e3o simult\u00e2nea de diferentes opera\u00e7\u00f5es em v\u00e1rias parti\u00e7\u00f5es de dados. Quando aplicamos uma transforma\u00e7\u00e3o ou a\u00e7\u00e3o em um RDD (Resilient Distributed Dataset) ou DataFrame no Spark, resulta-se em uma s\u00e9rie de tarefas que s\u00e3o processadas em paralelo nos diferentes n\u00f3s do cluster. Para aqueles familiarizados com o Airflow, a men\u00e7\u00e3o de DAG (Directed Acyclic Graph) trar\u00e1 lembran\u00e7as, o que faz sentido, uma vez que neste contexto, o n\u00edvel de paraleliza\u00e7\u00e3o \u00e9 uma DAG, mas no Spark.   A seguir, apresentamos uma representa\u00e7\u00e3o visual de como essa opera\u00e7\u00e3o se desenrola. Em etapas futuras, vamos explorar esse aspecto visualmente dentro do Spark. No entanto, neste instante inicial, nosso foco ser\u00e1 estritamente conceitual."},{"location":"arquitetura/#job","title":"Job","text":"Um job \u00e9 uma unidade de trabalho ou um conjunto de tarefas que precisam ser realizadas no cluster para cumprir uma opera\u00e7\u00e3o espec\u00edfica em um conjunto de dados. Quando executamos um aplicativo Spark, ele cria um plano de execu\u00e7\u00e3o l\u00f3gica para as transforma\u00e7\u00f5es definidas pelo usu\u00e1rio em seus RDDs ou DataFrames. Este plano \u00e9 representado como um DAG (Directed Acyclic Graph), mostrando a sequ\u00eancia de transforma\u00e7\u00f5es aplicadas aos dados para obter o resultado almejado.  Resumo: No Spark, um job \u00e9 uma unidade de trabalho que envolve transforma\u00e7\u00f5es realizadas em um conjunto de dados distribu\u00eddo, buscando alcan\u00e7ar um determinado resultado."},{"location":"arquitetura/#stage","title":"Stage","text":"No contexto do Apache Spark, um \"stage\" representa uma divis\u00e3o l\u00f3gica do plano de execu\u00e7\u00e3o que abrange uma s\u00e9rie de transforma\u00e7\u00f5es. Estas transforma\u00e7\u00f5es podem ser realizadas simultaneamente, sem a necessidade de transferir dados entre as parti\u00e7\u00f5es. Quando ativamos um aplicativo Spark, ele constr\u00f3i um plano de execu\u00e7\u00e3o l\u00f3gica, representado por um DAG (Directed Acyclic Graph), com base nas transforma\u00e7\u00f5es determinadas pelo usu\u00e1rio em seus RDDs. Este plano l\u00f3gico \u00e9 ent\u00e3o fragmentado em v\u00e1rios \"stages\", e cada \"stage\" compreende um conjunto de transforma\u00e7\u00f5es (ou tasks) que s\u00e3o processadas de forma paralela.  Resumo: No Spark, um \"stage\" \u00e9 uma divis\u00e3o l\u00f3gica do plano de execu\u00e7\u00e3o e simboliza um conjunto de transforma\u00e7\u00f5es processadas simultaneamente."},{"location":"arquitetura/#task","title":"Task","text":"Uma \"task\", no Spark, \u00e9 a menor unidade de execu\u00e7\u00e3o e representa uma opera\u00e7\u00e3o espec\u00edfica feita em uma parti\u00e7\u00e3o de dados. As tasks s\u00e3o a unidade b\u00e1sica de trabalho no Spark. Elas s\u00e3o enviadas aos executores do cluster para processamento paralelo. Mais especificamente, uma task \u00e9 destinada a um slot espec\u00edfico do executor.  Resumo: No Spark, uma \"task\" \u00e9 a menor unidade de trabalho e denota uma opera\u00e7\u00e3o espec\u00edfica realizada em uma parti\u00e7\u00e3o de dados."},{"location":"arquitetura/#conclusao","title":"Conclus\u00e3o","text":"Como se inter-relacionam o n\u00edvel de paraleliza\u00e7\u00e3o de dados e o n\u00edvel de tarefas? O menor componente no n\u00edvel de dados \u00e9 o slot do executor. Este recebe do driver tasks (trechos de c\u00f3digo) que s\u00e3o de sua responsabilidade executar em seu slot. No slot, dispomos de recursos computacionais, tais como CPU e mem\u00f3ria RAM. Para finalizar, para que o c\u00f3digo seja processado, precisamos de uma parti\u00e7\u00e3o do nosso RDD juntamente com uma a\u00e7\u00e3o."},{"location":"arquitetura/#referencias","title":"Refer\u00eancias","text":"<p>Getting Started with Apache Spark - Databricks What is Apache Spark Driver? - SparkByExamples Spark Basics: Application, Driver, Executor, Job, Stage and TaskWalkthrough - Kontext Understanding Spark Application Concepts - Knoldus Blog Spark Architecture 101 - Concepts &amp; Application - DevGenius Blog What Are Spark Applications? - Databricks Spark: Jobs, Stages and Tasks - Medium by Thisun S\u00e9rie Spark e Databricks - Parte 1: Arquitetura e Componentes do Apache</p>"},{"location":"boas_praticas/","title":"Boas Pr\u00e1ticas de Uso do Spark","text":"1. Uso de UDFs (User Defined Functions) Recomenda\u00e7\u00e3o: Evite o uso excessivo de UDFs. Eles podem ser menos otimizados do que as fun\u00e7\u00f5es nativas do Spark e, assim, reduzir o desempenho.  2. Compreens\u00e3o do Lazy Evaluation O Spark utiliza um conceito chamado \"Lazy Evaluation\", ou seja, ele n\u00e3o executa uma opera\u00e7\u00e3o at\u00e9 que seja absolutamente necess\u00e1rio. Recomenda\u00e7\u00e3o: Esteja ciente dessa caracter\u00edstica para evitar surpresas durante a execu\u00e7\u00e3o.  3. Uso de Cache Utilize .cache() para armazenar um objeto (como um DataFrame) na mem\u00f3ria. Isso pode acelerar opera\u00e7\u00f5es subsequentes no mesmo objeto. Use .uncache() para remover um objeto previamente armazenado em cache da mem\u00f3ria. Recomenda\u00e7\u00e3o: Use o cache sabiamente. N\u00e3o cache todos os DataFrames indiscriminadamente. Avalie quais objetos realmente beneficiariam do cache.  4. Configura\u00e7\u00f5es para Otimiza\u00e7\u00e3o de Mem\u00f3ria Ative spark.sql.inMemoryColumnarStorage.compressed para armazenamento colunar comprimido em mem\u00f3ria. Ao armazenar DataFrames em cache, prefira o formato colunar para otimizar o espa\u00e7o e a velocidade de acesso.  5. Otimiza\u00e7\u00f5es de Join Use \"Broadcast Join\" quando uma das tabelas em sua opera\u00e7\u00e3o join for pequena o suficiente para caber na mem\u00f3ria. A configura\u00e7\u00e3o spark.sql.autoBroadcastJoinThreshold determina o tamanho m\u00e1ximo de uma tabela que pode ser transmitida para todos os n\u00f3s durante um join. Ajuste conforme necess\u00e1rio. Dica: [Broadcast Hash Join with Spark DataFrame](https://stackoverflow.com/questions/47744265/broadcast-hash-join-with-spark-dataframe)  6. Ativa\u00e7\u00e3o de Otimizadores Ative o Otimizador Baseado em Custo (CBO) usando spark.sql.cbo.enabled. Ative o Adaptive Query Execution (AQE) com spark.sql.adaptive.enabled.  7. Gerenciamento de Parti\u00e7\u00f5es Equil\u00edbrio de Parti\u00e7\u00f5es: Poucas parti\u00e7\u00f5es podem resultar em alguns executores ociosos, enquanto muitas parti\u00e7\u00f5es podem levar \u00e0 sobrecarga de agendamento de tarefas. Encontrar o equil\u00edbrio certo \u00e9 essencial. Configura\u00e7\u00e3o de Parti\u00e7\u00f5es: Defina a quantidade e o tamanho ideal das parti\u00e7\u00f5es do seu RDD de acordo com o contexto do seu trabalho. Utilize a configura\u00e7\u00e3o spark.sql.shuffle.partitions (valor padr\u00e3o \u00e9 200) para ajustar conforme necess\u00e1rio. Redu\u00e7\u00e3o de Embaralhamento: O embaralhamento de dados pode ser uma opera\u00e7\u00e3o custosa. Tente minimiz\u00e1-la sempre que poss\u00edvel. Filtre Dados: Antes de realizar opera\u00e7\u00f5es, filtre seus dados para trabalhar apenas com as informa\u00e7\u00f5es necess\u00e1rias.  8. Evite Opera\u00e7\u00f5es Custosas Ordena\u00e7\u00e3o: Evite usar order by a menos que seja estritamente necess\u00e1rio. Sele\u00e7\u00e3o de Colunas: Em vez de usar select * para recuperar todas as colunas, especifique e recupere apenas as colunas relevantes para sua consulta.  9. Uso Eficiente da Reparti\u00e7\u00e3o e Coalesce Se desejar reduzir o n\u00famero de parti\u00e7\u00f5es, prefira usar coalesce(). \u00c9 uma vers\u00e3o otimizada em compara\u00e7\u00e3o com repartition(). O coalesce \u00e9 mais eficiente, especialmente com grandes conjuntos de dados, pois move menos dados entre parti\u00e7\u00f5es.  10. Minimize opera\u00e7\u00f5es com Shuffle e configure de paralelismo O shuffle pode ser uma opera\u00e7\u00e3o intensiva e pode afetar o desempenho. Portanto, tente evitar muitas opera\u00e7\u00f5es que envolvam shuffle. spark.default.parallelism: Define o n\u00famero padr\u00e3o de parti\u00e7\u00f5es em opera\u00e7\u00f5es de RDD sem shuffle, como parallelize, map e filter. O padr\u00e3o varia entre tipos de cluster. No local mode, o padr\u00e3o \u00e9 o n\u00famero de n\u00facleos dispon\u00edveis; em um cluster YARN, o padr\u00e3o \u00e9 o n\u00famero m\u00e1ximo de tarefas simult\u00e2neas que Spark pode executar em todo o cluster.  11. Configura\u00e7\u00e3o da Mem\u00f3ria Off-Heap A mem\u00f3ria off-heap refere-se \u00e0 aloca\u00e7\u00e3o de mem\u00f3ria fora da heap do Java, permitindo que o Spark use mem\u00f3ria al\u00e9m da mem\u00f3ria m\u00e1xima configurada para a JVM. Ativa\u00e7\u00e3o da Mem\u00f3ria Off-Heap: spark.memory.offHeap.enabled: Ativa o uso da mem\u00f3ria off-heap. Por padr\u00e3o, \u00e9 false. Para ativar, defina como true. spark.memory.offHeap.size: Define o tamanho da mem\u00f3ria off-heap. Deve ser especificado em bytes, ou voc\u00ea pode usar sufixos padr\u00e3o como \"k\", \"m\", \"g\", etc. (por exemplo, \"512m\").  12. Modificar o Compression Block Size A compress\u00e3o pode ajudar a reduzir a quantidade de dados sendo transmitidos entre as fases e, consequentemente, acelerar o embaralhamento. \u2022   Compression Block Size: \u2022   spark.io.compression.lz4.blockSize: Define o tamanho do bloco para a compress\u00e3o LZ4, quando essa compress\u00e3o est\u00e1 sendo usada. O tamanho padr\u00e3o \u00e9 32k.  13. Broadcasting Broadcasting \u00e9 uma t\u00e9cnica usada no Spark para otimizar opera\u00e7\u00f5es de join quando um dos DataFrames (ou RDDs) \u00e9 pequeno o suficiente para ser enviado para todos os n\u00f3s do cluster. \u2022   Usando Broadcasting: Carregue um pequeno conjunto de dados na mem\u00f3ria para juntar-se a um grande conjunto de dados. Isso evita o embaralhamento e pode acelerar significativamente as opera\u00e7\u00f5es de join."},{"location":"boas_praticas/#referencias","title":"Refer\u00eancias","text":"<p>Spark Shuffle Partitions vs Default Parallelism Towards Data Science - Apache Spark Configuration Data for Geeks - Apache Spark Performance Tuning Apache Spark Optimization Techniques by Ch-Nabarun Analytics Vidhya - Apache Spark Memory Management Dive into Spark Memory by Luminousmen Official Spark Configuration Documentation Spark by Examples - Spark Performance Tuning Spark by Examples - Spark SQL Performance Tuning</p>"},{"location":"broadcast/","title":"Broadcast Join","text":"O \"Broadcast Join\", tamb\u00e9m conhecido como \"Map-side Join\", \u00e9 uma t\u00e9cnica de otimiza\u00e7\u00e3o adotada pelo Apache Spark para eficientizar opera\u00e7\u00f5es de join entre dois conjuntos de dados, seja em DataFrames ou RDDs. Este m\u00e9todo \u00e9 particularmente \u00fatil quando um dos conjuntos de dados \u00e9 suficientemente pequeno para ser difundido por todos os n\u00f3s de um cluster.  Quando executamos opera\u00e7\u00f5es de join em sistemas distribu\u00eddos, um dos desafios \u00e9 a necessidade de redistribuir, ou \"shuffle\", os dados entre diferentes n\u00f3s. O objetivo do Broadcast Join \u00e9 minimizar ou, idealmente, eliminar essa redistribui\u00e7\u00e3o.  Funcionamento do Broadcast Join:  - Identifica\u00e7\u00e3o do DataFrame Pequeno: Esse m\u00e9todo \u00e9 mais eficaz quando um dos DataFrames, que denominamos de \"DataFrame de transmiss\u00e3o\" ou \"tabela menor\", tem um tamanho manej\u00e1vel para ser alocado na mem\u00f3ria de cada executor.  - Transmiss\u00e3o: Esse DataFrame menor \u00e9 serializado e ent\u00e3o enviado para todos os n\u00f3s do cluster. Isso assegura que cada n\u00f3 tenha uma c\u00f3pia desse DataFrame armazenada em sua mem\u00f3ria.  - Join Local: Com o DataFrame menor j\u00e1 distribu\u00eddo por todos os n\u00f3s, a opera\u00e7\u00e3o de join com o DataFrame maior acontece localmente em cada n\u00f3, eliminando a necessidade de redistribui\u00e7\u00e3o de dados entre os n\u00f3s.  Vantagens do Broadcast Join:  - Redu\u00e7\u00e3o no Shuffle: Ao minimizar a necessidade de redistribui\u00e7\u00e3o de dados entre n\u00f3s, o Broadcast Join pode agilizar significativamente opera\u00e7\u00f5es de join, otimizando a performance.  - Simplicidade: Em diversos cen\u00e1rios, o pr\u00f3prio Spark pode detectar automaticamente o tamanho do DataFrame e decidir por usar o Broadcast Join."},{"location":"broadcast/#referencias","title":"Refer\u00eancias","text":"<ul> <li>SparkByExamples - Broadcast Join</li> <li>Sujithjay - Broadcast Joins</li> <li>Blog Compass Uol - Estrat\u00e9gias de Join</li> </ul>"},{"location":"cache_persist/","title":"Cache e Persist","text":"cache() e persist() s\u00e3o m\u00e9todos do Apache Spark que permitem armazenar um DataFrame ou RDD na mem\u00f3ria, melhorando a performance de consultas e opera\u00e7\u00f5es subsequentes realizadas nesse DataFrame ou RDD. Vamos entender cada um:  cache(): <ul> <li>Quando voc\u00ea invoca o m\u00e9todo cache() em um DataFrame ou RDD, ele utiliza o n\u00edvel de armazenamento padr\u00e3o, que \u00e9 MEMORY_ONLY.</li> <li>Isso significa que o DataFrame ou RDD ser\u00e1 armazenado na mem\u00f3ria do executor e, se o espa\u00e7o da mem\u00f3ria for insuficiente para armazenar todos os dados, as parti\u00e7\u00f5es que n\u00e3o se encaixam na mem\u00f3ria n\u00e3o ser\u00e3o armazenadas em cache e ter\u00e3o que ser recalculadas quando necess\u00e1rio.</li> <li>Em termos simples, cache() \u00e9 um atalho para usar persist() com o n\u00edvel de armazenamento padr\u00e3o.</li> </ul> persist(): <ul> <li>O m\u00e9todo persist() permite que voc\u00ea especifique um n\u00edvel de armazenamento para o DataFrame ou RDD.</li> <li>Os n\u00edveis de armazenamento incluem:</li> <li>MEMORY_ONLY: Armazena o RDD ou DataFrame na mem\u00f3ria do executor. Se n\u00e3o houver mem\u00f3ria suficiente, algumas parti\u00e7\u00f5es n\u00e3o ser\u00e3o armazenadas em cache.</li> <li>MEMORY_ONLY_SER: Armazena o RDD ou DataFrame na mem\u00f3ria do executor como objetos serializados.</li> <li>MEMORY_AND_DISK: Armazena o RDD ou DataFrame na mem\u00f3ria e escreve as parti\u00e7\u00f5es que n\u00e3o se encaixam na mem\u00f3ria em disco.</li> <li>MEMORY_AND_DISK_SER: Semelhante ao MEMORY_AND_DISK, mas armazena os objetos como objetos serializados.</li> <li>E outros.</li> </ul>  Isso d\u00e1 uma flexibilidade maior em termos de onde e como os dados s\u00e3o armazenados em cache, dependendo dos recursos dispon\u00edveis e das necessidades da aplica\u00e7\u00e3o. Uso Pr\u00e1tico: Se voc\u00ea quer apenas armazenar um DataFrame ou RDD na mem\u00f3ria, usar cache() \u00e9 suficiente. No entanto, se quiser um controle mais detalhado sobre onde e como os dados s\u00e3o armazenados, ou se quiser armazenar em disco, usar persist() \u00e9 a melhor op\u00e7\u00e3o. Nota: \u00c9 importante lembrar que tanto cache() quanto persist() s\u00e3o opera\u00e7\u00f5es lazy no Spark, o que significa que o DataFrame ou RDD n\u00e3o ser\u00e1 realmente armazenado em cache at\u00e9 que uma a\u00e7\u00e3o seja executada nele (como count(), collect(), etc.)."},{"location":"cache_persist/#referencias","title":"Refer\u00eancias","text":"<p>Spark by Examples - Diferen\u00e7a entre cache() e persist() Documenta\u00e7\u00e3o do Spark - cache() LinkedIn - Spark cache() vs persist() Databricks - Melhores pr\u00e1ticas para cache e take</p>"},{"location":"catalyst_optimizer/","title":"Catalyst Optimizer","text":""},{"location":"catalyst_optimizer/#catalyst-optimizer_1","title":"Catalyst Optimizer","text":"Catalyst Optimizer do Spark SQL O Catalyst Optimizer \u00e9 um otimizador de consulta extens\u00edvel introduzido no Spark SQL para melhorar a performance de opera\u00e7\u00f5es SQL no Spark. Ele descobre automaticamente o plano mais eficiente para executar opera\u00e7\u00f5es de dados especificadas na consulta do usu\u00e1rio. Ele representa as consultas como \u00e1rvores l\u00f3gicas e aplica uma s\u00e9rie de otimiza\u00e7\u00f5es para transformar essas \u00e1rvores em planos de execu\u00e7\u00e3o f\u00edsicos mais eficientes.  Sua estrutura \u00e9 composta por \u00e1rvores l\u00f3gicas, seguida por otimiza\u00e7\u00f5es, depois \u00e9 elaborado uma \u00e1rvore f\u00edsica e, por fim, s\u00e3o realizadas otimiza\u00e7\u00f5es f\u00edsicas. E o que \u00e9 cada um desses elementos? <ul> <li>\u00c1rvore L\u00f3gica: A primeira etapa na otimiza\u00e7\u00e3o \u00e9 a convers\u00e3o da consulta em uma \u00e1rvore l\u00f3gica. Esta \u00e1rvore representa a estrutura da consulta, mas n\u00e3o detalha como ela deve ser executada.</li> <li>Otimiza\u00e7\u00f5es L\u00f3gicas: S\u00e3o aplicadas v\u00e1rias regras de otimiza\u00e7\u00e3o para melhorar a estrutura da \u00e1rvore l\u00f3gica. Isso inclui coisas como a propaga\u00e7\u00e3o de predicados e a elimina\u00e7\u00e3o de subconsultas redundantes.</li> <li>\u00c1rvore F\u00edsica: A \u00e1rvore l\u00f3gica otimizada \u00e9 ent\u00e3o transformada em uma \u00e1rvore f\u00edsica, que representa um plano de como a consulta ser\u00e1 realmente executada.</li> <li>Otimiza\u00e7\u00f5es F\u00edsicas: Aqui, o Spark considera diferentes maneiras de executar a consulta e escolhe a mais eficiente. Isso pode incluir escolher diferentes algoritmos de join ou decidir quando usar opera\u00e7\u00f5es de broadcast.</li> </ul> Caracter\u00edsticas Principais: <ul> <li>Generalidade: O Catalyst foi projetado para ser generalizado, permitindo que otimiza\u00e7\u00f5es adicionais sejam facilmente adicionadas no futuro. Ele tamb\u00e9m pode ser usado para otimizar pipelines al\u00e9m do SQL, como os do MLlib.</li> <li>Extensibilidade: O framework Catalyst \u00e9 altamente extens\u00edvel. Os desenvolvedores podem adicionar novas otimiza\u00e7\u00f5es ou at\u00e9 mesmo estender a linguagem SQL.</li> <li>Nativo em Scala: Catalyst \u00e9 escrito e constru\u00eddo no Scala, o que permite que ele opere de forma integrada com o c\u00f3digo Spark existente.</li> </ul> <p>Benef\u00edcios</p> <ul> <li>Performance: Atrav\u00e9s das otimiza\u00e7\u00f5es do Catalyst, as consultas SQL no Spark s\u00e3o frequentemente executadas muito mais rapidamente do que seriam sem ele.</li> <li>Flexibilidade: Como o Catalyst permite que os desenvolvedores introduzam novas otimiza\u00e7\u00f5es, ele torna o Spark SQL mais adapt\u00e1vel \u00e0s necessidades espec\u00edficas dos projetos.</li> </ul> <p> </p>"},{"location":"catalyst_optimizer/#referencias","title":"Refer\u00eancias","text":"<p>https://www.databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html https://medium.com/@Shkha_24/catalyst-optimizer-the-power-of-spark-sql-cad8af46097f https://www.databricks.com/glossary/catalyst-optimizer https://www.projectpro.io/recipes/explain-spark-catalyst-optimizer https://www.linkedin.com/pulse/apache-spark-catalyst-optimizer-shobhit-singh/</p>"},{"location":"configuracao_otimizacao_cluster/","title":"Configura\u00e7\u00e3o e Otimiza\u00e7\u00e3o do Cluster","text":""},{"location":"configuracao_otimizacao_cluster/#configuracao-e-otimizacao-do-cluster","title":"Configura\u00e7\u00e3o e Otimiza\u00e7\u00e3o do Cluster","text":"A import\u00e2ncia de configurar corretamente um cluster Spark \u00e9 ineg\u00e1vel. Uma configura\u00e7\u00e3o adequada \u00e9 crucial, j\u00e1 que ela impacta diretamente a performance, efici\u00eancia e escalabilidade quando se trata de processamento distribu\u00eddo de dados. Abordaremos os problemas que podem surgir de uma configura\u00e7\u00e3o inadequada e tamb\u00e9m os benef\u00edcios de um ajuste correto.  <p>Consequ\u00eancias de uma M\u00e1 Configura\u00e7\u00e3o:</p> <ul> <li>Queda de Performance: Performance comprometida pode significar perda de produtividade.</li> <li>Subutiliza\u00e7\u00e3o de Recursos: Recursos valiosos, como mem\u00f3ria e CPU, podem ser desperdi\u00e7ados.</li> <li>Falhas Frequentes: Configura\u00e7\u00f5es inadequadas podem causar instabilidades e falhas constantes.</li> <li>Aumento dos Custos: Em ambientes de nuvem, os custos podem escalar rapidamente com m\u00e1 configura\u00e7\u00e3o.</li> <li>Dificuldades no Debug: Diagn\u00f3stico de problemas torna-se mais complicado, levando mais tempo para resolu\u00e7\u00e3o.</li> <li>Limita\u00e7\u00f5es de Escalabilidade: Escalar para lidar com mais dados pode se tornar um desafio.</li> </ul> <p>Benef\u00edcios de uma Configura\u00e7\u00e3o Adequada:</p> <ul> <li>Performance Otimizada: Processamento mais \u00e1gil e eficiente de grandes volumes de dados.</li> <li>Utiliza\u00e7\u00e3o Eficiente de Recursos: Maximiza\u00e7\u00e3o dos recursos dispon\u00edveis e evitando desperd\u00edcios.</li> <li>Escalabilidade: Expans\u00e3o facilitada do cluster para atender a demandas crescentes.</li> <li>Confiabilidade: Maior estabilidade e recupera\u00e7\u00e3o \u00e1gil ap\u00f3s falhas.</li> <li>Gerenciamento Equilibrado: Distribui\u00e7\u00e3o uniforme da carga de trabalho entre os n\u00f3s do cluster.</li> <li>Menor Lat\u00eancia: Respostas mais \u00e1geis em opera\u00e7\u00f5es interativas, como consultas SQL.</li> </ul> Considera\u00e7\u00f5es ao Configurar um Cluster: <ul> <li>Volume de dados</li> <li>Quantidade de mem\u00f3ria dispon\u00edvel</li> <li>N\u00famero de cores de processamento</li> <li>Mem\u00f3ria alocada para o driver</li> <li>Mem\u00f3ria alocada para o executor</li> <li>Cores alocadas para o executor</li> <li>Cores alocadas para o driver</li> <li>Decis\u00e3o sobre armazenamento em disco ou na RAM</li> <li>Defini\u00e7\u00e3o da quantidade e tamanho ideal de parti\u00e7\u00f5es do RDD</li> <li>Configura\u00e7\u00e3o de mem\u00f3ria para serializa\u00e7\u00e3o</li> <li>Ativa\u00e7\u00e3o do Adaptive Query Execution (AQE) e do CBO quando necess\u00e1rio</li> <li>Ajuste do Compression Block Size</li> </ul>"},{"location":"configuracao_otimizacao_cluster/#o-que-utilizar-na-configuracao-do-meu-sparksession","title":"O que utilizar na configura\u00e7\u00e3o do meu sparkSession?","text":"O Spark oferece uma ampla variedade de configura\u00e7\u00f5es para otimizar e personalizar a sua sess\u00e3o conforme as necessidades. Embora existam muitas op\u00e7\u00f5es, focaremos em algumas configura\u00e7\u00f5es fundamentais e relevantes:  Configura\u00e7\u00e3o Descri\u00e7\u00e3o .master(\"local[*]\") Determina o modo de execu\u00e7\u00e3o para local. O [*] indica que o Spark deve usar todos os n\u00facleos dispon\u00edveis no sistema. spark.driver.cores Configura o n\u00famero de n\u00facleos usados pelo driver do Spark. spark.driver.memory Estabelece a quantidade de mem\u00f3ria a ser alocada pelo driver do Spark. spark.default.parallelism Define o paralelismo padr\u00e3o, o n\u00famero padr\u00e3o de parti\u00e7\u00f5es em opera\u00e7\u00f5es RDD sem shuffling (como map). spark.executor.cores Especifica o n\u00famero de n\u00facleos a serem utilizados por cada executor. spark.executor.instances Configura o n\u00famero de inst\u00e2ncias do executor. spark.executor.memory Define a quantidade de mem\u00f3ria para cada executor. spark.memory.fraction Estabelece a fra\u00e7\u00e3o da mem\u00f3ria total do heap para armazenamento e computa\u00e7\u00e3o. spark.memory.storageFraction Indica a fra\u00e7\u00e3o da mem\u00f3ria do Spark (baseado na configura\u00e7\u00e3o spark.memory.fraction) destinada ao cache de RDD. spark.memory.offHeap.enabled Ativa o uso de mem\u00f3ria off-heap. spark.memory.offHeap.size Define a capacidade da mem\u00f3ria off-heap. spark.sql.repl.eagerEval.enabled Habilita a avalia\u00e7\u00e3o ansiosa de DataFrames no REPL do Spark, \u00fatil para visualiza\u00e7\u00f5es durante o desenvolvimento. spark.sql.repl.eagerEval.maxNumRows Configura o n\u00famero m\u00e1ximo de linhas exibidas quando a avalia\u00e7\u00e3o ansiosa est\u00e1 ativa. .appName() Determina o nome da aplica\u00e7\u00e3o Spark. Configura\u00e7\u00f5es para GPUs (Nota: \u00e9 essencial instalar e configurar os drivers da GPU. Informe-se mais antes de usar): Configura\u00e7\u00e3o Descri\u00e7\u00e3o spark.plugins Lista os plugins a serem carregados. O com.nvidia.spark.SQLPlugin \u00e9 o plugin SQL da NVIDIA para Spark, possibilitando que o Spark execute opera\u00e7\u00f5es SQL em GPUs NVIDIA, acelerando consideravelmente opera\u00e7\u00f5es como joins e agrega\u00e7\u00f5es. spark.executor.resource.gpu.amount Define a quantidade de GPUs para cada executor. spark.rapids.memory.pinnedPool.size Estabelece o tamanho do pool de mem\u00f3ria \"pinned\", utilizada para transfer\u00eancias eficientes entre CPU e GPU."},{"location":"configuracao_otimizacao_cluster/#referencias","title":"Refer\u00eancias","text":"<p>Basics of Apache Spark Configuration Settings - Towards Data Science Apache Spark Performance Tuning and Best Practices - DataForGeeks Apache Spark Optimization Techniques - Medium Apache Spark Memory Management - Medium Dive into Spark Memory - LuminousMen Spark Configuration - Official Documentation Spark Performance Tuning - SparkByExamples Spark SQL Performance Tuning Configurations - SparkByExamples</p>"},{"location":"createorplacetempview/","title":"CreateOrReplaceTempView","text":""},{"location":"createorplacetempview/#createorreplacetempview","title":"CreateOrReplaceTempView","text":"Uma das principais vantagens do Apache Spark \u00e9 a capacidade de trabalhar com SQL juntamente com a API DataFrame/Dataset. Isso significa que, se voc\u00ea j\u00e1 estiver familiarizado com o SQL, poder\u00e1 criar uma exibi\u00e7\u00e3o tempor\u00e1ria de um DataFrame/Dataset utilizando o m\u00e9todo `createOrReplaceTempView()` e ent\u00e3o usar SQL para selecionar e manipular os dados. Para persistir uma exibi\u00e7\u00e3o tempor\u00e1ria como uma tabela, voc\u00ea pode usar `saveAsTable()`.   Aqui est\u00e1 um exemplo de como usar o `createOrReplaceTempView()`:  <p>df_dataset.createOrReplaceTempView(\"dataset\") df = spark.sql(\"select col_1, col2 from dataset\") df1.createOrReplaceTempView(\"df1\") df2.createOrReplaceTempView(\"df2\") df_email_enj = spark.sql(''' SELECT * FROM df1 LEFT JOIN df2 USING(column_key) ''')</p>"},{"location":"createorplacetempview/#referencias","title":"Refer\u00eancias","text":"<p>Documenta\u00e7\u00e3o Oficial do Spark Tutorial SparkbyExamples</p>"},{"location":"dag/","title":"Dica Olhe a DAG","text":"<p>Localiza\u00e7\u00e3o: Aba SQL/DataFrame</p>  A Directed Acyclic Graph (DAG) \u00e9 uma parte essencial do ecossistema Apache Spark, permitindo aos usu\u00e1rios visualizar o fluxo de tarefas e est\u00e1gios de um job. Esta representa\u00e7\u00e3o gr\u00e1fica n\u00e3o apenas ajuda a entender a sequ\u00eancia e a interdepend\u00eancia das opera\u00e7\u00f5es, mas tamb\u00e9m oferece v\u00e1rias vantagens para otimizar e depurar aplica\u00e7\u00f5es Spark. Abaixo est\u00e3o alguns dos benef\u00edcios mais not\u00e1veis \u200b\u200bda visualiza\u00e7\u00e3o DAG:  <ul> <li>Visualiza\u00e7\u00e3o do Fluxo de Processamento: A DAG apresenta uma clara representa\u00e7\u00e3o visual do fluxo de tarefas e est\u00e1gios, proporcionando uma compreens\u00e3o abrangente de como os dados s\u00e3o processados e transformados ao longo do job.</li> <li>Identifica\u00e7\u00e3o de Gargalos: Analisando a estrutura da DAG, os desenvolvedores podem identificar pontos de inefici\u00eancia ou gargalos, permitindo a otimiza\u00e7\u00e3o e melhor desempenho das aplica\u00e7\u00f5es.</li> <li>Depura\u00e7\u00e3o Facilitada: A visualiza\u00e7\u00e3o DAG serve como uma ferramenta valiosa para identificar rapidamente onde os problemas ou falhas podem ter ocorrido, simplificando o processo de depura\u00e7\u00e3o.</li> <li>Monitoramento de Otimiza\u00e7\u00f5es: A representa\u00e7\u00e3o DAG evidencia as transforma\u00e7\u00f5es feitas pelo otimizador Catalyst, permitindo que os usu\u00e1rios reconhe\u00e7am e compreendam os benef\u00edcios de otimiza\u00e7\u00f5es autom\u00e1ticas.</li> <li>Transpar\u00eancia na Execu\u00e7\u00e3o: A DAG fornece insights detalhados sobre o funcionamento interno do Spark, tornando o processo de execu\u00e7\u00e3o transparente e compreens\u00edvel, mesmo para aqueles menos familiarizados com o Spark.</li> <li>Avalia\u00e7\u00e3o de Estrat\u00e9gias de Agendamento: Atrav\u00e9s da interdepend\u00eancia e ordem das tarefas exibidas na DAG, os usu\u00e1rios podem avaliar e, se necess\u00e1rio, personalizar estrat\u00e9gias de agendamento para melhorar a efici\u00eancia.</li> </ul>  Ao incorporar a visualiza\u00e7\u00e3o DAG em suas pr\u00e1ticas de desenvolvimento e monitoramento, os usu\u00e1rios podem garantir um gerenciamento mais eficaz e uma execu\u00e7\u00e3o otimizada de seus jobs no Apache Spark.  <p> </p>"},{"location":"environment/","title":"Aba Environment","text":""},{"location":"environment/#environment-na-spark-ui","title":"Environment na Spark UI","text":"A aba de configura\u00e7\u00e3o na Spark UI oferece um panorama das v\u00e1rias configura\u00e7\u00f5es e propriedades associadas \u00e0 aplica\u00e7\u00e3o Spark em execu\u00e7\u00e3o. Especificamente, nesta aba, voc\u00ea encontrar\u00e1 as seguintes informa\u00e7\u00f5es:  <ul> <li>Configura\u00e7\u00f5es de Aplica\u00e7\u00e3o: Abrange as configura\u00e7\u00f5es espec\u00edficas da aplica\u00e7\u00e3o Spark, incluindo qualquer par\u00e2metro definido pelo usu\u00e1rio e as configura\u00e7\u00f5es padr\u00e3o do Spark.</li> <li>Configura\u00e7\u00f5es do Sistema: Fornecem detalhes sobre propriedades do sistema Java, tais como a vers\u00e3o do Java, os caminhos do classpath, entre outros aspectos pertinentes ao ambiente Java em execu\u00e7\u00e3o.</li> <li>Depend\u00eancias de Classpath: Lista todas as bibliotecas e depend\u00eancias presentes no classpath da aplica\u00e7\u00e3o. Esta se\u00e7\u00e3o \u00e9 particularmente \u00fatil para depura\u00e7\u00e3o e valida\u00e7\u00e3o de vers\u00f5es de bibliotecas.</li> <li>Propriedades de Configura\u00e7\u00e3o: Oferece uma vis\u00e3o detalhada de todas as propriedades de configura\u00e7\u00e3o do Spark, destacando tanto as configura\u00e7\u00f5es padr\u00e3o quanto aquelas modificadas ou especificamente definidas pelo usu\u00e1rio.</li> </ul>  Ao se familiarizar com essa aba, os usu\u00e1rios ganham uma compreens\u00e3o mais clara das nuances de configura\u00e7\u00e3o de sua aplica\u00e7\u00e3o Spark, permitindo otimiza\u00e7\u00f5es e ajustes conforme necess\u00e1rio."},{"location":"environment/#referencias","title":"Refer\u00eancias","text":"<p>The Internals of Spark Core UI - Guia de um Mestre Documenta\u00e7\u00e3o Oficial da Spark UI</p>"},{"location":"executors/","title":"Aba Executors","text":""},{"location":"executors/#executors-na-spark-ui","title":"Executors na Spark UI","text":"A Spark UI, com sua aba \"Executors\", desempenha um papel fundamental na monitoriza\u00e7\u00e3o e gest\u00e3o dos executores associados a uma aplica\u00e7\u00e3o Spark. Esta aba \u00e9 especialmente \u00fatil para obter insights sobre o desempenho e a sa\u00fade dos executores no contexto do cluster Spark. Ela fornece uma representa\u00e7\u00e3o visual clara de diversos m\u00e9tricas e detalhes essenciais sobre os executores ativos, bem como sobre aqueles que foram removidos ou que falharam.   Veja uma breve descri\u00e7\u00e3o do que voc\u00ea encontrar\u00e1 na aba \"Executors\":  <ul> <li>Executor ID: \u00c9 o identificador exclusivo atribu\u00eddo a cada executor, ajudando na identifica\u00e7\u00e3o e rastreamento do mesmo.</li> <li>Address: Refere-se ao endere\u00e7o do n\u00f3 em que o executor est\u00e1 ou estava rodando, fornecendo informa\u00e7\u00f5es sobre a localiza\u00e7\u00e3o f\u00edsica do executor no cluster.</li> <li>Task metrics: Esta se\u00e7\u00e3o apresenta v\u00e1rias m\u00e9tricas relacionadas \u00e0s tarefas de cada executor, como o n\u00famero total de tarefas, falhas e a dura\u00e7\u00e3o total.</li> <li>Shuffle metrics: Aqui, os usu\u00e1rios podem encontrar informa\u00e7\u00f5es valiosas sobre opera\u00e7\u00f5es de shuffle, incluindo shuffle read e shuffle write.</li> <li>Storage metrics: Proporciona uma vis\u00e3o detalhada do uso da mem\u00f3ria e do armazenamento em disco por cada executor, ajudando a identificar poss\u00edveis gargalos ou problemas relacionados ao armazenamento.</li> <li>Logs: Para cada executor, s\u00e3o fornecidos links diretos para os seus logs padr\u00e3o e de erros, facilitando o diagn\u00f3stico e a resolu\u00e7\u00e3o de problemas.</li> </ul>  Monitorar e entender as m\u00e9tricas e detalhes fornecidos pela aba \"Executors\" \u00e9 crucial para garantir que os recursos do cluster Spark estejam sendo utilizados de forma eficiente e para identificar e resolver rapidamente quaisquer problemas que possam surgir."},{"location":"executors/#referencias","title":"Refer\u00eancias","text":"<p>The Internals of Spark Core UI - Guia de um Mestre Documenta\u00e7\u00e3o Oficial da Spark UI</p>"},{"location":"funcoes_importante/","title":"Pratique Pyspark","text":""},{"location":"funcoes_importante/#funcoes-importantes","title":"Fun\u00e7\u00f5es Importantes","text":"Fun\u00e7\u00e3o Descri\u00e7\u00e3o Categoria .sort() Ordena o DataFrame. Desconhecido .orderBy() Ordena o DataFrame baseado em colunas. Desconhecido .desc_null_first() Ordena de forma decrescente tratando NULLs primeiro. Desconhecido .asc_null_first() Ordena de forma crescente tratando NULLs primeiro. Desconhecido .desc_null_last() Ordena de forma decrescente colocando NULLs por \u00faltimo. Desconhecido .asc_null_last() Ordena de forma crescente colocando NULLs por \u00faltimo. Desconhecido .coalesce() Reduz o n\u00famero de parti\u00e7\u00f5es. Narrow .repartition() Reparticiona o DataFrame em n parti\u00e7\u00f5es. Wide .cache() Armazena o DataFrame na mem\u00f3ria. A\u00e7\u00e3o .unpersist() Remove o DataFrame do armazenamento em cache. A\u00e7\u00e3o .storageLevel() Retorna o n\u00edvel de armazenamento do DataFrame. Desconhecido .filter()/.where() Filtra as linhas do DataFrame. Narrow .select() Seleciona colunas espec\u00edficas. Narrow .agg() Agrega dados. Wide .groupBy() Agrupa os dados por colunas espec\u00edficas. Wide .drop() Remove uma ou mais colunas. Narrow .dropDuplicates() Remove linhas duplicadas. Wide .sample() Retorna uma amostra do DataFrame. Narrow .UDF() Aplica uma fun\u00e7\u00e3o definida pelo usu\u00e1rio. Desconhecido .foreach() Aplica uma fun\u00e7\u00e3o a cada linha do DataFrame. A\u00e7\u00e3o .when()/.otherwise() Funcionalidade condicional. Narrow .isNull() Verifica se um valor \u00e9 NULL. Narrow .window() Utilizado para fun\u00e7\u00f5es de janela. Desconhecido .summary() Retorna estat\u00edsticas resumidas. Wide .describe() Retorna estat\u00edsticas descritivas. Wide .union() Une dois DataFrames. Narrow .unionByName() Une dois DataFrames por nome. Narrow .join() Realiza um join entre dois DataFrames. Wide .na.drop() Remove linhas com valores NA. Narrow .na.fill() Preenche valores NA com valores espec\u00edficos. Narrow .row() Retorna uma linha do DataFrame. Desconhecido .col() Retorna uma coluna do DataFrame. Desconhecido .withColumn() Adiciona ou substitui uma coluna. Narrow .withColumnRenamed() Renomeia uma coluna. Narrow .approx_count_distinct() Retorna a contagem aproximada de valores distintos. Wide .array_contains() Verifica se um array cont\u00e9m um valor espec\u00edfico. Narrow .contains() Verifica se uma string cont\u00e9m outra string. Narrow .distinct() Retorna valores distintos. Wide .size() Retorna o tamanho de uma coluna de tipo array. Narrow .explode() Transforma cada elemento de uma coluna de array em uma linha. Wide .explode_outer() Semelhante ao .explode() mas preserva linhas com NULL. Wide .posexplode() Semelhante ao .explode() mas adiciona uma coluna com \u00edndice. Wide .take() Retorna os primeiros n elementos. A\u00e7\u00e3o .collect() Retorna todos os registros do DataFrame como uma lista. A\u00e7\u00e3o .isNotNull() Verifica se um valor n\u00e3o \u00e9 NULL. Narrow .isin() Verifica se um valor est\u00e1 em uma lista de valores. Narrow createOrReplaceTempView() Cria ou substitui uma vis\u00e3o tempor\u00e1ria. A\u00e7\u00e3o + spark.sql('''codigo_sql''') Executa uma consulta SQL e retorna um DataFrame. A\u00e7\u00e3o .count() Retorna a contagem de linhas. A\u00e7\u00e3o .sum() Retorna a soma dos valores. A\u00e7\u00e3o .avg() Retorna a m\u00e9dia dos valores. A\u00e7\u00e3o .min()/max() Retorna o valor m\u00ednimo/m\u00e1ximo. A\u00e7\u00e3o .countApprox() Retorna uma contagem aproximada de valores distintos. A\u00e7\u00e3o .count_distinct() Retorna a contagem de valores distintos. A\u00e7\u00e3o .alias() Define um alias para o DataFrame. Narrow .cast() Converte o tipo de coluna. Narrow .regex_replace() Substitui substrings baseado em express\u00e3o regular. Narrow .regex_extract() Extrai substrings usando express\u00e3o regular. Narrow .startswith() Verifica se uma string come\u00e7a com uma substring. Narrow .endswith() Verifica se uma string termina com uma substring. Narrow .month() Retorna o m\u00eas de uma data. Narrow .hour() Retorna a hora de uma data. Narrow .year() Retorna o ano de uma data. Narrow .unix_timestamp() Converte a data para um timestamp Unix. Narrow .from_unixtime() Retorna a data a partir de um timestamp Unix. Narrow .from_utc_timestamp() Retorna a data a partir de um UTC timestamp. Narrow .datediff() Retorna a diferen\u00e7a entre duas datas. Narrow .to_date() Converte uma string para uma data. Narrow .current_date() Retorna a data atual. Narrow .lit() Retorna uma coluna de valor constante. Narrow"},{"location":"funcoes_importante/#formas-diferentes-de-escrever-a-mesma-coisa","title":"Formas diferentes de escrever a mesma coisa","text":"No Spark, \u00e9 poss\u00edvel expressar a mesma opera\u00e7\u00e3o de v\u00e1rias maneiras, gra\u00e7as \u00e0 sua flexibilidade e design voltado ao usu\u00e1rio. Abaixo, apresento exemplos dessa versatilidade usando fun\u00e7\u00f5es comuns, como filtragem, joins e ordena\u00e7\u00e3o.  Filter: <ul> <li><code>df.filter(df.column != \u2018xxxxx\u2019)</code></li> <li><code>df.filter(~(df.column == \u2018xxxxx\u2019))</code></li> <li><code>df.filter(col(\u2018column\u2019) != \u2018xxxxx\u2019)</code></li> <li><code>df.filter(column != \u2018xxxxxx</code>)`</li> </ul> Joins: <ul> <li><code>df1.join(df2, col(\u2018df1.column\u2019) == col(\u2018df2.column\u2019), \u2018left\u2019)</code></li> <li><code>df1.join(df2, df1.column == df2.column, \u2018left\u2019)</code></li> <li><code>df1.join(df2, df1[\u2018column\u2019] == df2[\u2018column\u2019], \u2018left\u2019)</code></li> <li><code>df1.join(df2, [\u2018column\u2019, \u2018column\u2019], \u2018left\u2019)</code> </li> </ul> sort e orderBy: <ul> <li><code>df.sort(df.column.desc())</code></li> <li><code>df.sort(col(\u2018column\u2019).desc())</code></li> <li><code>df.sort(desc(\u2018column\u2019))</code></li> <li><code>df.sort(desc(col(\u2018column\u2019)))</code></li> <li><code>df.sort(col(\u2018column\u2019), ascending = false)</code></li> </ul>"},{"location":"funcoes_importante/#referencia","title":"Refer\u00eancia","text":"<p>Documenta\u00e7\u00e3o Pyspark</p>"},{"location":"instalacao_local/","title":"Instalacao Local","text":"A inten\u00e7\u00e3o neste segmento n\u00e3o \u00e9 detalhar o passo a passo da instala\u00e7\u00e3o do Spark, uma vez que h\u00e1 uma vasta gama de recursos dispon\u00edveis online que j\u00e1 abordam este t\u00f3pico de maneira competente. Neste espa\u00e7o, busco fazer observa\u00e7\u00f5es espec\u00edficas sobre as precau\u00e7\u00f5es a serem tomadas durante o processo de instala\u00e7\u00e3o e configura\u00e7\u00e3o. Se voc\u00ea est\u00e1 em busca de instru\u00e7\u00f5es detalhadas, vou listar alguns sites que considerei \u00fateis ao realizar minha pr\u00f3pria instala\u00e7\u00e3o.  O que preciso baixar? <ul> <li>Spark - Apache Official</li> <li>Hadoop - Winutils</li> <li>Java (vers\u00e3o 8 ou superior) - Oracle </li> </ul> <p>Cuidados</p> <ul> <li>Garanta que o PATH do Spark, Hadoop e Java estejam corretamente configurados.</li> <li>Ap\u00f3s instalar os componentes necess\u00e1rios, execute o comando pip install pyspark.</li> </ul>"},{"location":"instalacao_local/#referencias","title":"Refer\u00eancias","text":"<p>Instala\u00e7\u00e3o de PySpark - Medium Executando PySpark no Windows - SparkByExamples Guia de Instala\u00e7\u00e3o do Spark - CrayonData </p>"},{"location":"job/","title":"Aba Job","text":"<p> <p> </p> <p>Os trabalhos s\u00e3o clic\u00e1veis \u200b\u200b(e fornecem informa\u00e7\u00f5es sobre as etapas das tarefas contidas nele). Ao passar o mouse sobre um trabalho na Linha do tempo do evento, voc\u00ea n\u00e3o apenas v\u00ea a legenda do trabalho, mas tamb\u00e9m o trabalho \u00e9 destacado na se\u00e7\u00e3o Resumo.   A se\u00e7\u00e3o Event Timeline mostra jobs e executores.  </p>"},{"location":"job/#referencias","title":"Refer\u00eancias","text":"<p>Documenta\u00e7\u00e3o Oficial da Spark UI The Internals of Spark Core UI - Guia de um Mestre</p>"},{"location":"lazy_evaluation/","title":"O Lazy Evaluation?","text":"Lazy Evaluation (ou \"Avalia\u00e7\u00e3o Pregui\u00e7osa\") \u00e9 um conceito em programa\u00e7\u00e3o e computa\u00e7\u00e3o onde a avalia\u00e7\u00e3o de uma express\u00e3o \u00e9 adiada at\u00e9 que o valor da express\u00e3o seja efetivamente necess\u00e1rio. Isso pode levar a otimiza\u00e7\u00f5es significativas de desempenho, reduzindo a quantidade de c\u00e1lculos realizados e, em alguns casos, evitando completamente certas avalia\u00e7\u00f5es. Quando usamos as fun\u00e7\u00f5es select, filter ou groupBy no Spark, o driver armazena as informa\u00e7\u00f5es dessas transforma\u00e7\u00f5es. Estas s\u00e3o categorizadas como 'narrow' e 'wide' transformations no Spark (abordaremos esse tema mais adiante). O resultado destas transforma\u00e7\u00f5es s\u00f3 ser\u00e1 retornado ao executarmos a\u00e7\u00f5es, como count, collect ou max, que demandam uma resposta imediata para o usu\u00e1rio. E quais os benef\u00edcios Lazy Evaluation (avalia\u00e7\u00e3o pregui\u00e7osa)? - Melhora a efici\u00eancia: O otimizador Catalyst do Spark agrupar\u00e1 as opera\u00e7\u00f5es, reduzindo o n\u00famero de repasses de dados e melhorando o desempenho. - Melhor Legibilidade: Como voc\u00ea sabe que o Spark agrupar\u00e1 as opera\u00e7\u00f5es e otimizar\u00e1 o c\u00f3digo nos bastidores, voc\u00ea pode organizar seu programa usando opera\u00e7\u00f5es menores que melhorar\u00e3o a legibilidade e a manuten\u00e7\u00e3o do c\u00f3digo. - Gerenciamento de mem\u00f3ria: Se as transforma\u00e7\u00f5es do Spark fossem \u00e1vidas, voc\u00ea teria que armazenar todos os Dataframes/RDDs intermedi\u00e1rios em algum lugar ou, no m\u00ednimo, gerenciar a mem\u00f3ria se tornaria outra preocupa\u00e7\u00e3o sua."},{"location":"lazy_evaluation/#referencias","title":"Refer\u00eancias","text":"<p>Explain Spark Lazy Evaluation in Detail Lazy Evaluation in Apache Spark 3 Reasons Why Spark's Lazy Evaluation is Useful</p>"},{"location":"memoria_serializacao/","title":"Mem\u00f3ria de Serializa\u00e7\u00e3o","text":"A serializa\u00e7\u00e3o \u00e9 o processo pelo qual os objetos de dados s\u00e3o convertidos em uma s\u00e9rie de bytes, permitindo que esses dados sejam facilmente transmitidos pela rede ou armazenados para uso posterior. No contexto do Apache Spark, essa t\u00e9cnica desempenha um papel crucial. Quando os dados s\u00e3o transferidos entre os executores ou entre um executor e o driver, esses dados s\u00e3o serializados.  O Spark fornece dois tipos principais de serializadores:  <ul> <li> <p>Java Serializer: Este \u00e9 o serializador padr\u00e3o do Spark e \u00e9 conhecido por sua capacidade de serializar qualquer objeto. No entanto, ele pode n\u00e3o ser o mais eficiente em termos de velocidade ou tamanho de sa\u00edda.</p> </li> <li> <p>Kryo Serializer: Muito mais eficiente que o serializador Java, o Kryo \u00e9 recomendado quando a velocidade de serializa\u00e7\u00e3o e a minimiza\u00e7\u00e3o do tamanho dos dados serializados s\u00e3o cruciais. No entanto, nem todos os objetos podem ser serializados com o Kryo por padr\u00e3o, e pode ser necess\u00e1rio registrar classes espec\u00edficas.</p> </li> </ul>  Ao considerar a utiliza\u00e7\u00e3o do Spark em ambientes de produ\u00e7\u00e3o, especialmente em aplica\u00e7\u00f5es que demandam alta performance, \u00e9 essencial avaliar qual serializador se encaixa melhor nas necessidades da aplica\u00e7\u00e3o. A escolha correta pode resultar em ganhos significativos de performance e efici\u00eancia.  Por fim, \u00e9 importante destacar que a mem\u00f3ria utilizada para serializa\u00e7\u00e3o \u00e9 cr\u00edtica, especialmente durante opera\u00e7\u00f5es de shuffle, uma vez que os dados precisam ser serializados antes de serem transferidos pela rede.  <p> </p>"},{"location":"memoria_serializacao/#referencias","title":"Refer\u00eancias","text":"<p>SparkbyExamples - MaxResultSize</p>"},{"location":"pyspark_cheat_sheet/","title":"Pyspark Cheat Sheet","text":"<p>Documenta\u00e7\u00e3o Oficial do PySpark PySpark Cheat Sheet no GitHub DataCamp PySpark SQL Cheat Sheet (PDF)</p>"},{"location":"rdd_spark/","title":"O que \u00e9 um RDD e um DataFrame Spark?","text":""},{"location":"rdd_spark/#o-que-e-um-rdd","title":"O que \u00e9 um RDD?","text":"Um RDD (Resilient Distributed Dataset) \u00e9 um conjunto imut\u00e1vel de itens de dados distribu\u00eddos por v\u00e1rios n\u00f3s de um cluster, que pode ser manipulado atrav\u00e9s de APIs para realizar transforma\u00e7\u00f5es e diversas outras a\u00e7\u00f5es. A estrutura do RDD foi projetada para ser:  <ul> <li>Resili\u00eancia: Como o nome sugere, RDDs s\u00e3o resilientes, o que significa que podem se recuperar automaticamente de falhas. A resili\u00eancia \u00e9 alcan\u00e7ada atrav\u00e9s da reten\u00e7\u00e3o de informa\u00e7\u00f5es sobre como um RDD pode ser reconstru\u00eddo (normalmente um plano de linhagem) em caso de perda de dados.</li> <li>Distribu\u00eddo: RDDs est\u00e3o distribu\u00eddos por v\u00e1rios n\u00f3s em um cluster, permitindo processamento paralelo.</li> <li>Dataset: Representa uma cole\u00e7\u00e3o (ou conjunto) de dados.  </li> </ul>"},{"location":"rdd_spark/#o-que-e-um-dataframe-spark","title":"O que \u00e9 um DataFrame Spark","text":"No Spark, DataFrames representam conjuntos de dados distribu\u00eddos organizados em linhas e colunas nomeadas. Semelhante \u00e0s tabelas de bancos de dados tradicionais, cada coluna em um DataFrame tem um nome espec\u00edfico e um tipo de dado associado. DataFrames s\u00e3o constru\u00eddos sobre o RDD, que \u00e9 a abstra\u00e7\u00e3o de dados mais fundamental do Spark. Embora compartilhem caracter\u00edsticas com bancos de dados relacionais, DataFrames possuem t\u00e9cnicas avan\u00e7adas de otimiza\u00e7\u00e3o, tornando o processamento mais eficiente."},{"location":"rdd_spark/#ilustrando-com-um-meme-o-que-e-seria-um-rdd","title":"Ilustrando com um meme o que \u00e9 seria um RDD","text":""},{"location":"rdd_spark/#referencias","title":"Refer\u00eancias","text":"<p>Spark Fundamentals - Medium Different ways to define the structure of DataFrame using Spark StructType - ProjectPro What are Resilient Distributed Datasets (RDD)? - phoenixNAP</p>"},{"location":"referencias/","title":"Outros Links","text":""},{"location":"referencias/#referencias","title":"Refer\u00eancias","text":""},{"location":"referencias/#conceitos-gerais-e-guia-de-estudo","title":"Conceitos Gerais e Guia de Estudo","text":"<p>Guia/Documenta\u00e7\u00e3o de um dos membros fundadores Relato de como passar na certifica\u00e7\u00e3o 1 Relato de como passar na certifica\u00e7\u00e3o 2 LinkedIn - Study Guide Databricks Certified Medium - Complete Guide to Crack Databricks Certified Medium - Tips on Cracking the Databricks Spark Developer Certification</p>"},{"location":"referencias/#curso-e-simulados","title":"Curso e Simulados","text":"<p>Udemy - Apache Spark 3 Databricks Certified Course Databricks Practice Exam PDF Udemy - Apache Spark 3 Databricks Certified Quiz</p>"},{"location":"referencias/#material-de-leitura","title":"Material de Leitura","text":"<p>Learning Spark Lightning-Fast Data Analytics</p>"},{"location":"referencias/#documentacao-apache-spark-pyspark","title":"Documenta\u00e7\u00e3o Apache Spark (Pyspark)","text":"<p>RDD Programming Guide - Spark 3.5.0 Apache Spark SQL and DataFrames Guide Spark 3.5.0 Pyspark</p>"},{"location":"referencias/#ferramenta-de-estudo","title":"Ferramenta de Estudo","text":"<p>OpenAI Chat - GPT4</p>"},{"location":"referencias/#documentacoes-e-tutoriais-adicionais","title":"Documenta\u00e7\u00f5es e Tutoriais Adicionais","text":"<p>Databricks Blog Spark Tutorials - DataFlair</p>"},{"location":"shuffle/","title":"Shuffle","text":""},{"location":"shuffle/#shuffle","title":"Shuffle","text":"No contexto do Apache Spark, o termo \"shuffle\" se refere ao processo de redistribui\u00e7\u00e3o de dados entre parti\u00e7\u00f5es. Esta etapa \u00e9 crucial quando a reorganiza\u00e7\u00e3o ou o agrupamento de dados conforme chaves espec\u00edficas \u00e9 requerido, como nas opera\u00e7\u00f5es de agrupamento ou \"join\". O shuffle \u00e9 not\u00f3rio por ser uma das opera\u00e7\u00f5es mais custosas em termos computacionais dentro do Spark, visto que demanda a transfer\u00eancia de dados entre diversos executores do cluster.  A aus\u00eancia de um shuffle poderia resultar em parti\u00e7\u00f5es com tamanhos desbalanceados, prejudicando o desempenho global da aplica\u00e7\u00e3o."},{"location":"shuffle/#como-determinar-a-quantidade-de-shuffles","title":"Como determinar a quantidade de Shuffles?","text":"Para estimar a quantidade \u00f3tima de shuffles, sobretudo em rela\u00e7\u00e3o ao par\u00e2metro spark.sql.shuffle.partitions, os seguintes crit\u00e9rios devem ser considerados:  Para Datasets Grandes: Recomenda-se determinar o tamanho alvo da tarefa entre 100 MB e no m\u00e1ximo 200 MB por parti\u00e7\u00e3o.  F\u00f3rmula Recomendada: Para definir spark.sql.shuffle.partitions utiliza-se: (tamanho de entrada do stage de shuffle / tamanho-alvo por parti\u00e7\u00e3o) / total de n\u00facleos) * total de n\u00facleos."},{"location":"shuffle/#consequencias-de-um-shuffle-ineficiente","title":"Consequ\u00eancias de um Shuffle Ineficiente","text":"Um shuffle mal gerenciado pode levar a diversas consequ\u00eancias indesej\u00e1veis, incluindo:  <p>Aumento no Tempo de Execu\u00e7\u00e3o: Opera\u00e7\u00f5es extensivas de shuffle podem ser muito demoradas, em especial se os dados estiverem distribu\u00eddos de maneira desequilibrada entre as parti\u00e7\u00f5es.  Consumo Elevado de Recursos: Shuffles massivos podem demandar grande parte da CPU e mem\u00f3ria, interferindo em outros jobs ou aplica\u00e7\u00f5es que estejam rodando no mesmo cluster. </p>"},{"location":"shuffle/#boas-praticas","title":"Boas pr\u00e1ticas","text":"Para maximizar a efici\u00eancia do shuffle, algumas pr\u00e1ticas s\u00e3o recomendadas:  <p>Minimize Opera\u00e7\u00f5es que Provocam Shuffle: Se poss\u00edvel, organize suas transforma\u00e7\u00f5es de forma a reduzir as opera\u00e7\u00f5es que induzam ao shuffle.  Ajuste o Tamanho das Parti\u00e7\u00f5es: Adaptar o tamanho das parti\u00e7\u00f5es pode ajudar a balancear a distribui\u00e7\u00e3o de tarefas entre os executores e reduzir o overhead associado ao shuffle.  </p>"},{"location":"shuffle/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Understanding Apache Spark Shuffle - Medium</li> <li>Apache Spark Shuffle Service - Ksolves</li> <li>Shuffling in Apache Spark - Mikulski Bartosz</li> <li>Spark Shuffle - SparkByExample</li> </ul>"},{"location":"sobre_certificacao/","title":"Sobre a certifica\u00e7\u00e3o","text":""},{"location":"sobre_certificacao/#certificacao","title":"Certifica\u00e7\u00e3o","text":"Dura\u00e7\u00e3o do teste: Voc\u00ea ter\u00e1 120 minutos para fazer o exame de certifica\u00e7\u00e3o.  Quantidade de quest\u00f5es: H\u00e1 60 quest\u00f5es de m\u00faltipla escolha no exame de certifica\u00e7\u00e3o.  Segmenta\u00e7\u00e3o das Quest\u00f5es: - Conceitos de Arquitetura do Apache Spark: 17% (10/60) - Aplicativos de arquitetura Apache Spark: 11% (7/60) - Aplicativos da API Apache Spark DataFrame: 72% (43/60) Valor do investimento: US$ 200 (sim, d\u00f3lares) Linguagem de programa\u00e7\u00e3o usada na certifica\u00e7\u00e3o: A certifica\u00e7\u00e3o pode ser feita em Python ou Scala. L\u00edngua que o teste \u00e9 aplicado: Ingl\u00eas M\u00e9todo de Realiza\u00e7\u00e3o: Online O que \u00e9 necess\u00e1rio saber sobre a realiza\u00e7\u00e3o do teste: Kryterion Support (empresa respons\u00e1vel por aplicar os testes) Dura\u00e7\u00e3o da certifica\u00e7\u00e3o: 2 anos \u00c9 permitido consulta a documenta\u00e7\u00e3o na hora do teste? Sim Dicas:  - N\u00e3o utilize notebook de empresa - Teste o notebook e o ambiente um dia antes"},{"location":"sobre_certificacao/#origem-das-informacoes","title":"Origem das informa\u00e7\u00f5es","text":"<p>Databricks Certifica\u00e7\u00e3o</p>"},{"location":"spark_ui/","title":"Spark UI","text":"A Spark UI, parte integrante do ecossistema Apache Spark, serve como uma poderosa interface para monitorar aplica\u00e7\u00f5es em execu\u00e7\u00e3o no Spark. Esta interface gr\u00e1fica oferece insights detalhados sobre o desempenho, estado das aplica\u00e7\u00f5es e muito mais, desempenhando um papel vital na otimiza\u00e7\u00e3o e monitoriza\u00e7\u00e3o do processamento distribu\u00eddo de dados, proporcionando uma vis\u00e3o completa das atividades do cluster.  Aspectos Importantes da Spark UI: <ul> <li>Interface Web Intuitiva: Acess\u00edvel via navegador, \u00e9 projetada para uma navega\u00e7\u00e3o f\u00e1cil, mesmo para usu\u00e1rios menos t\u00e9cnicos.</li> <li>Monitoriza\u00e7\u00e3o em Tempo Real: Fornece um acompanhamento em tempo real do progresso das aplica\u00e7\u00f5es Spark.</li> <li>Detalhes de Tarefas e Est\u00e1gios: Informa\u00e7\u00f5es sobre cada tarefa e est\u00e1gio, incluindo tempo gasto, n\u00famero de registros processados e outros.</li> <li>Visualiza\u00e7\u00e3o de Plano F\u00edsico e L\u00f3gico: Representa\u00e7\u00e3o gr\u00e1fica dos planos da aplica\u00e7\u00e3o, elucidando a execu\u00e7\u00e3o das opera\u00e7\u00f5es.</li> <li>Monitoriza\u00e7\u00e3o de Armazenamento em Cache: Ferramenta para monitorar o tamanho e efic\u00e1cia do armazenamento em cache.</li> <li>Hist\u00f3rico de Execu\u00e7\u00f5es: Manuten\u00e7\u00e3o do hist\u00f3rico de execu\u00e7\u00f5es passadas para an\u00e1lise comparativa.</li> <li>Configura\u00e7\u00e3o e Op\u00e7\u00f5es de Diagn\u00f3stico: Informa\u00e7\u00f5es sobre a configura\u00e7\u00e3o do cluster e detalhes da aplica\u00e7\u00e3o, auxiliando no ajuste de desempenho.</li> </ul>  O uso da Spark UI \u00e9 fundamental para aqueles envolvidos no universo Apache Spark. Esta ferramenta essencial permite que desenvolvedores e administradores obtenham insights valiosos, otimizem aplica\u00e7\u00f5es, solucionem problemas de desempenho e gerenciem clusters de processamento de dados de maneira eficiente. A Spark UI fortalece o arsenal do Apache Spark, tornando o processamento de big data mais transparente e gerenci\u00e1vel."},{"location":"spark_ui/#referencias","title":"Refer\u00eancias","text":"<p>Documenta\u00e7\u00e3o Oficial da Spark UI Guia de Uso da Spark UI - Medium The Internals of Spark Core UI</p>"},{"location":"stages/","title":"Aba Stages","text":""},{"location":"stages/#stages-na-spark-ui","title":"Stages na Spark UI","text":"A guia stage \u00e9 uma componente essencial da Spark UI, proporcionando uma vis\u00e3o abrangente e consolidada de todos os stages que comp\u00f5em os trabalhos do aplicativo Spark. Esta guia \u00e9 especialmente \u00fatil para os desenvolvedores e administradores que buscam monitorar, analisar e otimizar o desempenho das suas aplica\u00e7\u00f5es Spark em um n\u00edvel granular.  Subse\u00e7\u00f5es da Guia stages: <ul> <li>Tarefas e Estat\u00edsticas de stage: Ao selecionar um stage espec\u00edfico, os usu\u00e1rios s\u00e3o conduzidos a esta subse\u00e7\u00e3o. Ela apresenta informa\u00e7\u00f5es detalhadas sobre as tarefas associadas ao stage selecionado e oferece m\u00e9tricas e estat\u00edsticas cruciais relacionadas ao desempenho do stage.</li> <li>Detalhes do Pool: Esta subse\u00e7\u00e3o se torna acess\u00edvel quando o aplicativo Spark opera no modo de agendamento FAIR. Fornecendo insights valiosos sobre a configura\u00e7\u00e3o e o desempenho dos pools de recursos, ela \u00e9 vital para a gest\u00e3o eficaz dos recursos no Spark.</li> </ul> <p>Aviso</p> <p>\u00c9 importante notar que se nenhum trabalho foi submetido ao aplicativo Spark at\u00e9 o momento, a guia stages simplesmente mostrar\u00e1 seu t\u00edtulo, indicando que ainda n\u00e3o h\u00e1 stages para exibir. Esta caracter\u00edstica auxilia na limpeza visual e na usabilidade da Spark UI.</p>  A p\u00e1gina \"Stages\" na Spark UI \u00e9 essencial para monitorar e compreender os diferentes est\u00e1gios pelos quais um aplicativo Spark passa durante sua execu\u00e7\u00e3o. Esta p\u00e1gina classifica e apresenta os est\u00e1gios com base em seu estado atual, oferecendo uma vis\u00e3o clara e organizada do progresso e dos poss\u00edveis gargalos ou problemas que possam surgir.  Os est\u00e1gios de um aplicativo Spark s\u00e3o divididos em quatro categorias principais:  <ul> <li>Stages Ativos: Est\u00e1gios que est\u00e3o atualmente em execu\u00e7\u00e3o. Estes s\u00e3o os est\u00e1gios que o Spark est\u00e1 processando no momento.</li> <li>Etapas Pendentes: Est\u00e1gios que est\u00e3o na fila e aguardam a disponibilidade de recursos ou a conclus\u00e3o de est\u00e1gios dependentes.</li> <li>Etapas Conclu\u00eddas: Est\u00e1gios que foram processados e conclu\u00eddos com sucesso, sem erros.</li> <li>Fases Falhadas: Est\u00e1gios que encontraram erros durante sua execu\u00e7\u00e3o e n\u00e3o puderam ser conclu\u00eddos.</li> </ul> <p> </p>"},{"location":"stages/#referencias","title":"Refer\u00eancias","text":"<p>The Internals of Spark Core UI - Guia de um Mestre Documenta\u00e7\u00e3o Oficial da Spark UI</p>"},{"location":"storage/","title":"Aba Storage","text":""},{"location":"storage/#storage-na-spark-ui","title":"Storage na Spark UI","text":"A aba \"Storage\" da Spark UI \u00e9 um recurso valioso que fornece insights detalhados sobre o armazenamento e gerenciamento dos RDDs (Resilient Distributed Datasets) e DataFrames durante a execu\u00e7\u00e3o de uma aplica\u00e7\u00e3o Spark. Esta aba \u00e9 especialmente \u00fatil para monitorar a efic\u00e1cia do armazenamento em cache e para otimizar o desempenho do aplicativo, ajustando as estrat\u00e9gias de armazenamento em cache conforme necess\u00e1rio.   Aqui est\u00e1 uma vis\u00e3o geral das informa\u00e7\u00f5es chave que voc\u00ea encontrar\u00e1 na aba \"Storage\":  <ul> <li>RDD Name / DataFrame Name: Serve como identificador, mostrando o nome ou ID do RDD ou DataFrame. \u00c9 poss\u00edvel clicar no nome para acessar detalhes mais espec\u00edficos sobre o dataset em quest\u00e3o.</li> <li>Storage Level: Mostra o n\u00edvel de armazenamento definido para o RDD ou DataFrame. Isso pode variar de MEMORY_ONLY a MEMORY_AND_DISK, entre outras op\u00e7\u00f5es.</li> <li>Cached Partitions: Indica quantas parti\u00e7\u00f5es de um RDD ou DataFrame est\u00e3o armazenadas em cache.</li> <li>Fraction Cached: Apresenta uma representa\u00e7\u00e3o percentual, ilustrando a por\u00e7\u00e3o das parti\u00e7\u00f5es que foram armazenadas em cache.</li> <li>Size in Memory: Destaca o espa\u00e7o total em mem\u00f3ria que o RDD ou DataFrame ocupa.</li> <li>Size on Disk: Se relevante, mostra o espa\u00e7o total no disco usado pelo RDD ou DataFrame.</li> <li>Executors: Fornece detalhes sobre quais executores mant\u00eam partes do RDD ou DataFrame e o espa\u00e7o consumido por cada um.</li> </ul>  Esta aba \u00e9 um componente vital da Spark UI, auxiliando desenvolvedores e administradores a monitorar o armazenamento, identificar gargalos e tomar decis\u00f5es informadas para otimizar a efici\u00eancia do armazenamento em cache."},{"location":"storage/#referencias","title":"Refer\u00eancias","text":"<p>The Internals of Spark Core UI - Guia de um Mestre Documenta\u00e7\u00e3o Oficial da Spark UI</p>"},{"location":"tolerancia_falhas/","title":"Spark Tolerante a falhas","text":""},{"location":"tolerancia_falhas/#spark-tolerante-a-falhas","title":"Spark Tolerante a Falhas","text":"Mesmo operando em um ambiente distribu\u00eddo, com cada n\u00f3 trabalhando de forma independente, o Spark \u00e9 reconhecido pela sua capacidade de ser tolerante a falhas. Como mencionado em se\u00e7\u00f5es anteriores, o RDD \u00e9 um dos pilares dessa resili\u00eancia. Aqui, vamos nos aprofundar em outros mecanismos que proporcionam essa robustez ao Spark:  1. Resilient Distributed Datasets (RDDs): O RDD, a pedra angular do Spark, consiste em conjuntos de dados imut\u00e1veis que s\u00e3o particionados e distribu\u00eddos. Cada parti\u00e7\u00e3o \u00e9 replicada, pelo menos, em duas m\u00e1quinas diferentes no cluster. Assim, caso haja uma falha em um dos n\u00f3s, as parti\u00e7\u00f5es podem ser recriadas a partir das c\u00f3pias presentes em outros n\u00f3s.  2. Transforma\u00e7\u00f5es Determin\u00edsticas: As a\u00e7\u00f5es executadas sobre os RDDs no Spark s\u00e3o determin\u00edsticas. Ou seja, dado um conjunto de dados e uma sequ\u00eancia espec\u00edfica de transforma\u00e7\u00f5es, o resultado final ser\u00e1 sempre o mesmo. Isso \u00e9 crucial para a resili\u00eancia, pois, em caso de falhas em uma task, ela pode ser realocada e reexecutada em outro n\u00f3, assegurando a integridade do resultado.  3. Informa\u00e7\u00f5es de Linhagem (Lineage): O Spark mant\u00e9m um registro das transforma\u00e7\u00f5es aplicadas a um RDD, formando sua \"linhagem\". Se uma parti\u00e7\u00e3o for perdida ou corrompida, o Spark pode reconstru\u00ed-la atrav\u00e9s das opera\u00e7\u00f5es originais realizadas nos dados prim\u00e1rios.  4. Recupera\u00e7\u00e3o de Falhas: Se um n\u00f3 apresentar falhas durante o processamento, o Spark pode recuperar as parti\u00e7\u00f5es perdidas atrav\u00e9s da linhagem e redistribuir as tarefas para outros n\u00f3s dispon\u00edveis.  5. Dados em Mem\u00f3ria e Disco: O Spark pode guardar dados tanto na mem\u00f3ria quanto no disco. Se os dados estiverem na mem\u00f3ria e houver uma falha, o Spark pode regener\u00e1-los a partir dos RDDs originais. J\u00e1 se estiverem armazenados no disco, as parti\u00e7\u00f5es perdidas s\u00e3o reprocessadas com base nas transforma\u00e7\u00f5es anteriores.  6. Replica\u00e7\u00e3o de Dados: Al\u00e9m da replica\u00e7\u00e3o padr\u00e3o das parti\u00e7\u00f5es de RDDs, o Spark tamb\u00e9m permite a replica\u00e7\u00e3o de dados em caches ou diretamente na mem\u00f3ria. Isso acentua a disponibilidade de dados mesmo em cen\u00e1rios adversos."},{"location":"transformacaos_acoes/","title":"Trasnforma\u00e7\u00f5es e A\u00e7\u00f5es","text":""},{"location":"transformacaos_acoes/#transformacoes","title":"Transforma\u00e7\u00f5es","text":"O que s\u00e3o e quais as diferen\u00e7as entre transforma\u00e7\u00f5es e a\u00e7\u00f5es? Em uma se\u00e7\u00e3o anterior, discutimos a diferen\u00e7a entre transforma\u00e7\u00f5es e a\u00e7\u00f5es no Spark, especialmente no que tange \u00e0 avalia\u00e7\u00e3o \"lazy\". Agora, vamos nos aprofundar ainda mais nessa distin\u00e7\u00e3o.  Come\u00e7ando pelas opera\u00e7\u00f5es de transforma\u00e7\u00e3o no Spark, identificamos dois principais tipos: as transforma\u00e7\u00f5es narrow e as transforma\u00e7\u00f5es wide.  Transforma\u00e7\u00f5es Narrow: Nestas, os dados necess\u00e1rios para computar uma parti\u00e7\u00e3o espec\u00edfica j\u00e1 est\u00e3o presentes na pr\u00f3pria parti\u00e7\u00e3o. Alguns exemplos de tais opera\u00e7\u00f5es incluem select, filter, map, flatMap, sample e union.  Transforma\u00e7\u00f5es Wide: Aqui, os dados requeridos para processar uma parti\u00e7\u00e3o espec\u00edfica podem estar espalhados por diversas parti\u00e7\u00f5es. Algumas das opera\u00e7\u00f5es que se encaixam neste tipo s\u00e3o groupByKey, repartition, reduceByKey, distinct, join e coalesce.  <p> ! </p>"},{"location":"transformacaos_acoes/#acoes","title":"A\u00e7\u00f5es","text":"Em contraste, as a\u00e7\u00f5es s\u00e3o opera\u00e7\u00f5es que exigem um c\u00e1lculo imediato e s\u00e3o categorizadas como \"eager\" (ansiosas). Exemplos destas opera\u00e7\u00f5es englobam comandos como show, count, collect e save. \u00c9 essencial observar que um script no Spark s\u00f3 come\u00e7ar\u00e1 a executar transforma\u00e7\u00f5es (sejam elas narrow ou wide) ap\u00f3s uma a\u00e7\u00e3o ser invocada. Da mesma maneira, eventuais erros s\u00f3 ser\u00e3o detectados ap\u00f3s a execu\u00e7\u00e3o dessa a\u00e7\u00e3o. Abaixo temos dois exemplos de quando uma a\u00e7\u00e3o \u00e9 utilizada.  <p> <p> </p>"},{"location":"transformacaos_acoes/#referencias","title":"Refer\u00eancias","text":"<p>Wide vs Narrow Transformations in Spark/Distributed Compute What are Transformations? - Databricks Understanding Transformations vs Actions and Narrow vs Wide Transformations Spark: The Definitive Guide (In Short)</p>"}]}